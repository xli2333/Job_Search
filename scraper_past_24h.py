import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import random
import re 
from datetime import datetime
import os

def scrape_jobs(progress_callback=None):
    # --- 1. é…ç½®å‚æ•° ---
    search_url = "https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search"
    landing_url = "https://www.linkedin.com/jobs/search"
    detail_base_url = "https://www.linkedin.com/jobs-guest/jobs/api/jobPosting/{}"

    params = {
        "keywords": "Graduate",
        "location": "New York",
        "geoId": "105080838",
        "f_TPR": "r86400", # è¿‡å»24å°æ—¶
        "start": 0
    }

    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept-Language": "en-US,en;q=0.9",
    }

    max_jobs = 50 
    
    def log_progress(current, total, message):
        print(f"[{current}/{total}] {message}")
        if progress_callback:
            progress_callback(current, total, message)

    # --- è·å–èŒä½æ€»æ•°å¹¶è‡ªåŠ¨è®¾ç½®ç›®æ ‡ ---
    log_progress(0, 0, "æ­£åœ¨åˆ†æèŒä½æ€»æ•°...")
    try:
        landing_params = params.copy()
        if 'start' in landing_params:
            del landing_params['start']
            
        resp_total = requests.get(landing_url, params=landing_params, headers=headers, timeout=10)
        soup_total = BeautifulSoup(resp_total.text, 'html.parser')
        
        count_elem = soup_total.find("span", class_="results-context-header__job-count")
        
        if count_elem:
            total_text = count_elem.text.strip()
            match = re.search(r'(\d[\d,]*)', total_text)
            
            if match:
                total_count = int(match.group(1).replace(',', ''))
                print(f"ğŸ“Š LinkedIn æ˜¾ç¤ºå…±æœ‰: ã€{total_count}ã€‘ ä¸ªèŒä½")
                
                safety_limit = 300
                if safety_limit and total_count > safety_limit:
                    max_jobs = safety_limit
                    log_progress(0, max_jobs, f"èŒä½è¿‡å¤šï¼Œé™åˆ¶ä¸º {max_jobs} ä¸ª")
                else:
                    max_jobs = total_count
                    log_progress(0, max_jobs, f"ç›®æ ‡è®¾ç½®ä¸º {max_jobs} ä¸ª")
            else:
                 log_progress(0, max_jobs, "æ— æ³•æå–æ•°å­—ï¼Œä½¿ç”¨é»˜è®¤ç›®æ ‡")
        else:
            log_progress(0, max_jobs, "æœªèƒ½æå–æ€»æ•°ï¼Œä½¿ç”¨é»˜è®¤ç›®æ ‡")
            
    except Exception as e:
        log_progress(0, max_jobs, f"è·å–æ€»æ•°å¤±è´¥: {e}")

    print("-" * 30)

    # --- å¼€å§‹å¾ªç¯æŠ“å– ---
    job_list = []
    log_progress(0, max_jobs, "å¼€å§‹æŠ“å–...")

    while len(job_list) < max_jobs:
        try:
            log_progress(len(job_list), max_jobs, f"è¯·æ±‚åˆ—è¡¨é¡µ (start={params['start']})...")
            response = requests.get(search_url, params=params, headers=headers, timeout=10)
            
            if response.status_code != 200:
                log_progress(len(job_list), max_jobs, f"åˆ—è¡¨è¯·æ±‚å—é™: {response.status_code}")
                break

            soup = BeautifulSoup(response.text, 'html.parser')
            jobs = soup.find_all("li")

            if not jobs:
                log_progress(len(job_list), max_jobs, "æ²¡æœ‰æ›´å¤šèŒä½äº†")
                break

            print(f"   -> æœ¬é¡µè·å–åˆ° {len(jobs)} ä¸ªèŒä½...")

            for job in jobs:
                if len(job_list) >= max_jobs:
                    break
                
                try:
                    title = job.find("h3", class_="base-search-card__title").text.strip()
                    company = job.find("h4", class_="base-search-card__subtitle").text.strip()
                    location = job.find("span", class_="job-search-card__location").text.strip()
                    date = job.find("time").text.strip() if job.find("time") else "N/A"
                    link_tag = job.find("a", class_="base-card__full-link")
                    link = link_tag['href'].split('?')[0] if link_tag else "N/A"

                    base_card_div = job.find("div", class_="base-card")
                    job_id = ""
                    if base_card_div and 'data-entity-urn' in base_card_div.attrs:
                        urn = base_card_div['data-entity-urn']
                        job_id = urn.split(":")[-1]
                    
                    description = "N/A"
                    if job_id:
                        log_progress(len(job_list), max_jobs, f"æ­£åœ¨è·å–è¯¦æƒ…: {title[:15]}...")
                        target_api = detail_base_url.format(job_id)
                        desc_resp = requests.get(target_api, headers=headers, timeout=5)
                        
                        if desc_resp.status_code == 200:
                            desc_soup = BeautifulSoup(desc_resp.text, 'html.parser')
                            desc_div = desc_soup.find("div", class_="show-more-less-html__markup")
                            if desc_div:
                                description = desc_div.get_text(separator='\n').strip()
                        
                        time.sleep(random.uniform(0.5, 1.5))

                    job_list.append({
                        "Title": title,
                        "Company": company,
                        "Location": location,
                        "Date": date,
                        "Link": link,
                        "Job ID": job_id,
                        "Description": description
                    })
                    
                    log_progress(len(job_list), max_jobs, f"å·²è·å–: {title[:15]}")

                except Exception as e:
                    print(f"   âŒ è§£æå‡ºé”™: {e}")
                    continue 

            jobs_found_on_this_page = len(jobs)
            if jobs_found_on_this_page > 0:
                params['start'] += jobs_found_on_this_page
            else:
                params['start'] += 25
                
            time.sleep(random.uniform(2, 4))

        except Exception as e:
            log_progress(len(job_list), max_jobs, f"å‘ç”Ÿé”™è¯¯: {e}")
            break

    # --- ä¿å­˜ç»“æœ ---
    log_progress(len(job_list), max_jobs, "æŠ“å–å®Œæˆ")
    return job_list  # Return data directly, do not save file

if __name__ == "__main__":
    data = scrape_jobs()
    print(f"Scraped {len(data)} jobs locally.")